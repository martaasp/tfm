{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import functions\n",
    "from mappings import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from itertools import chain\n",
    "from functools import reduce\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "from pyspark.sql import Window, SparkSession, Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, StandardScaler, VectorAssembler\n",
    "from pyspark.ml.classification import LinearSVC, LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Spark\n",
    "\n",
    "We will create our Spark program using **SparkSession** ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f8fa0027350>\n",
      "<SparkContext master=yarn appName=my-tfm>\n"
     ]
    }
   ],
   "source": [
    "appName = \"my-tfm\"\n",
    "\n",
    "ss = SparkSession.builder.appName(appName).config(\"spark.some.config.option\", \"4g\").getOrCreate()\n",
    "print(ss)\n",
    "sc = ss.sparkContext\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading\n",
    "\n",
    "We will load the data previously downloaded from its source:\n",
    "\n",
    "* **Historical data:** http://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008\n",
    "\n",
    "* **Prediction data:** https://www.kaggle.com/kumargh/pimaindiansdiabetescsv\n",
    "\n",
    "* **Analysis (time-series):** http://archive.ics.uci.edu/ml/datasets/Diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22-08-2020 23:55:27 - Loading diabetic_data.csv file....\n",
      "Loaded 101766 rows and 50 columns\n",
      "+------------+-----------+---------+------+------+------+-----------------+------------------------+-------------------+----------------+----------+--------------------+------------------+--------------+---------------+-----------------+----------------+----------------+------+------+------+----------------+-------------+---------+---------+-----------+-----------+--------------+-----------+-------------+---------+---------+-----------+------------+-------------+--------+--------+------------+----------+-------+-----------+-------+-------------------+-------------------+------------------------+-----------------------+----------------------+------+-----------+----------+\n",
      "|encounter_id|patient_nbr|     race|gender|   age|weight|admission_type_id|discharge_disposition_id|admission_source_id|time_in_hospital|payer_code|   medical_specialty|num_lab_procedures|num_procedures|num_medications|number_outpatient|number_emergency|number_inpatient|diag_1|diag_2|diag_3|number_diagnoses|max_glu_serum|A1Cresult|metformin|repaglinide|nateglinide|chlorpropamide|glimepiride|acetohexamide|glipizide|glyburide|tolbutamide|pioglitazone|rosiglitazone|acarbose|miglitol|troglitazone|tolazamide|examide|citoglipton|insulin|glyburide-metformin|glipizide-metformin|glimepiride-pioglitazone|metformin-rosiglitazone|metformin-pioglitazone|change|diabetesMed|readmitted|\n",
      "+------------+-----------+---------+------+------+------+-----------------+------------------------+-------------------+----------------+----------+--------------------+------------------+--------------+---------------+-----------------+----------------+----------------+------+------+------+----------------+-------------+---------+---------+-----------+-----------+--------------+-----------+-------------+---------+---------+-----------+------------+-------------+--------+--------+------------+----------+-------+-----------+-------+-------------------+-------------------+------------------------+-----------------------+----------------------+------+-----------+----------+\n",
      "|     2278392|    8222157|Caucasian|Female|[0-10)|  null|                6|                      25|                  1|               1|      null|Pediatrics-Endocr...|                41|             0|              1|                0|               0|               0|250.83|  null|  null|               1|         None|     None|       No|         No|         No|            No|         No|           No|       No|       No|         No|          No|           No|      No|      No|          No|        No|     No|         No|     No|                 No|                 No|                      No|                     No|                    No|    No|         No|        NO|\n",
      "+------------+-----------+---------+------+------+------+-----------------+------------------------+-------------------+----------------+----------+--------------------+------------------+--------------+---------------+-----------------+----------------+----------------+------+------+------+----------------+-------------+---------+---------+-----------+-----------+--------------+-----------+-------------+---------+---------+-----------+------------+-------------+--------+--------+------------+----------+-------+-----------+-------+-------------------+-------------------+------------------------+-----------------------+----------------------+------+-----------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "22-08-2020 23:55:31 - Load completed. This operation has taken 4.51 s\n"
     ]
    }
   ],
   "source": [
    "# Load first dataset (historical data)\n",
    "hist_data = functions.load_data(ss, \"diabetic_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22-08-2020 23:55:31 - Loading pima-indians-diabetes.csv file....\n",
      "Loaded 768 rows and 9 columns\n",
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-----+\n",
      "|Pregnancies|Glucose|BloodPressure|SkinThickness|Insulin| BMI|DiabetesPedigreeFunction|Age|Class|\n",
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-----+\n",
      "|          6|    148|           72|           35|      0|33.6|                   0.627| 50|    1|\n",
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-----+\n",
      "only showing top 1 row\n",
      "\n",
      "22-08-2020 23:55:32 - Load completed. This operation has taken 0.65 s\n"
     ]
    }
   ],
   "source": [
    "# Load second dataset (prediction data)\n",
    "col_names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness',\\\n",
    "            'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Class']\n",
    "pred_data = functions.load_data(ss, \"pima-indians-diabetes.csv\", hdr=False, col_names=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22-08-2020 23:55:32 - Loading data-01 file....\n",
      "Loaded 943 rows and 5 columns\n",
      "22-08-2020 23:55:40 - Loading data-02 file....\n",
      "Loaded 761 rows and 5 columns\n",
      "22-08-2020 23:55:41 - Loading data-03 file....\n",
      "Loaded 300 rows and 5 columns\n",
      "22-08-2020 23:55:42 - Loading data-04 file....\n",
      "Loaded 300 rows and 5 columns\n",
      "22-08-2020 23:55:42 - Loading data-05 file....\n",
      "Loaded 300 rows and 5 columns\n",
      "22-08-2020 23:55:43 - Loading data-06 file....\n",
      "Loaded 149 rows and 5 columns\n",
      "22-08-2020 23:55:43 - Loading data-07 file....\n",
      "Loaded 242 rows and 5 columns\n",
      "22-08-2020 23:55:44 - Loading data-08 file....\n",
      "Loaded 177 rows and 5 columns\n",
      "22-08-2020 23:55:44 - Loading data-09 file....\n",
      "Loaded 206 rows and 5 columns\n",
      "22-08-2020 23:55:45 - Loading data-10 file....\n",
      "Loaded 247 rows and 5 columns\n",
      "22-08-2020 23:55:45 - Loading data-11 file....\n",
      "Loaded 236 rows and 5 columns\n",
      "22-08-2020 23:55:46 - Loading data-12 file....\n",
      "Loaded 300 rows and 5 columns\n",
      "22-08-2020 23:55:46 - Loading data-13 file....\n",
      "Loaded 300 rows and 5 columns\n",
      "22-08-2020 23:55:47 - Loading data-14 file....\n",
      "Loaded 230 rows and 5 columns\n",
      "22-08-2020 23:55:47 - Loading data-15 file....\n",
      "Loaded 300 rows and 5 columns\n",
      "22-08-2020 23:55:48 - Loading data-16 file....\n",
      "Loaded 300 rows and 5 columns\n",
      "22-08-2020 23:55:48 - Loading data-17 file....\n",
      "Loaded 251 rows and 5 columns\n",
      "22-08-2020 23:55:49 - Loading data-18 file....\n",
      "Loaded 300 rows and 5 columns\n",
      "22-08-2020 23:55:49 - Loading data-19 file....\n",
      "Loaded 300 rows and 5 columns\n",
      "22-08-2020 23:55:50 - Loading data-20 file....\n",
      "Loaded 1003 rows and 5 columns\n",
      "22-08-2020 23:55:50 - Loading data-21 file....\n",
      "Loaded 517 rows and 5 columns\n",
      "22-08-2020 23:55:51 - Loading data-22 file....\n",
      "Loaded 300 rows and 5 columns\n",
      "22-08-2020 23:55:52 - Loading data-23 file....\n",
      "Loaded 300 rows and 5 columns\n",
      "22-08-2020 23:55:52 - Loading data-24 file....\n",
      "Loaded 300 rows and 5 columns\n",
      "22-08-2020 23:55:53 - Loading data-25 file....\n",
      "Loaded 110 rows and 5 columns\n",
      "22-08-2020 23:55:53 - Loading data-26 file....\n",
      "Loaded 483 rows and 5 columns\n",
      "22-08-2020 23:55:54 - Loading data-27 file....\n",
      "Loaded 926 rows and 5 columns\n",
      "22-08-2020 23:55:54 - Loading data-28 file....\n",
      "Loaded 951 rows and 5 columns\n",
      "22-08-2020 23:55:55 - Loading data-29 file....\n",
      "Loaded 1289 rows and 5 columns\n",
      "22-08-2020 23:55:55 - Loading data-30 file....\n",
      "Loaded 1179 rows and 5 columns\n",
      "22-08-2020 23:55:55 - Loading data-31 file....\n",
      "Loaded 670 rows and 5 columns\n",
      "22-08-2020 23:55:56 - Loading data-32 file....\n",
      "Loaded 157 rows and 5 columns\n",
      "22-08-2020 23:55:56 - Loading data-33 file....\n",
      "Loaded 300 rows and 5 columns\n",
      "22-08-2020 23:55:57 - Loading data-34 file....\n",
      "Loaded 300 rows and 5 columns\n",
      "22-08-2020 23:55:57 - Loading data-35 file....\n",
      "Loaded 300 rows and 5 columns\n",
      "22-08-2020 23:55:58 - Loading data-36 file....\n",
      "Loaded 265 rows and 5 columns\n",
      "22-08-2020 23:55:58 - Loading data-37 file....\n",
      "Loaded 300 rows and 5 columns\n",
      "22-08-2020 23:56:00 - Loading data-38 file....\n",
      "Loaded 300 rows and 5 columns\n",
      "22-08-2020 23:56:01 - Loading data-39 file....\n",
      "Loaded 300 rows and 5 columns\n",
      "22-08-2020 23:56:04 - Loading data-40 file....\n",
      "Loaded 106 rows and 5 columns\n",
      "22-08-2020 23:56:04 - Loading data-41 file....\n",
      "Loaded 494 rows and 5 columns\n",
      "22-08-2020 23:56:05 - Loading data-42 file....\n",
      "Loaded 541 rows and 5 columns\n",
      "22-08-2020 23:56:05 - Loading data-43 file....\n",
      "Loaded 300 rows and 5 columns\n",
      "22-08-2020 23:56:06 - Loading data-44 file....\n",
      "Loaded 300 rows and 5 columns\n",
      "22-08-2020 23:56:06 - Loading data-45 file....\n",
      "Loaded 300 rows and 5 columns\n",
      "22-08-2020 23:56:07 - Loading data-46 file....\n",
      "Loaded 300 rows and 5 columns\n",
      "22-08-2020 23:56:07 - Loading data-47 file....\n",
      "Loaded 300 rows and 5 columns\n",
      "22-08-2020 23:56:08 - Loading data-48 file....\n",
      "Loaded 300 rows and 5 columns\n",
      "22-08-2020 23:56:09 - Loading data-49 file....\n",
      "Loaded 240 rows and 5 columns\n",
      "22-08-2020 23:56:09 - Loading data-50 file....\n",
      "Loaded 300 rows and 5 columns\n",
      "22-08-2020 23:56:09 - Loading data-51 file....\n",
      "Loaded 300 rows and 5 columns\n",
      "22-08-2020 23:56:10 - Loading data-52 file....\n",
      "Loaded 300 rows and 5 columns\n",
      "22-08-2020 23:56:11 - Loading data-53 file....\n",
      "Loaded 297 rows and 5 columns\n",
      "22-08-2020 23:56:16 - Loading data-54 file....\n",
      "Loaded 779 rows and 5 columns\n",
      "22-08-2020 23:56:18 - Loading data-55 file....\n",
      "Loaded 1327 rows and 5 columns\n",
      "22-08-2020 23:56:19 - Loading data-56 file....\n",
      "Loaded 1018 rows and 5 columns\n",
      "22-08-2020 23:56:19 - Loading data-57 file....\n",
      "Loaded 133 rows and 5 columns\n",
      "22-08-2020 23:56:20 - Loading data-58 file....\n",
      "Loaded 300 rows and 5 columns\n",
      "22-08-2020 23:56:21 - Loading data-59 file....\n",
      "Loaded 300 rows and 5 columns\n",
      "22-08-2020 23:56:21 - Loading data-60 file....\n",
      "Loaded 300 rows and 5 columns\n",
      "22-08-2020 23:56:21 - Loading data-61 file....\n",
      "Loaded 300 rows and 5 columns\n",
      "22-08-2020 23:56:22 - Loading data-62 file....\n",
      "Loaded 300 rows and 5 columns\n",
      "22-08-2020 23:56:22 - Loading data-63 file....\n",
      "Loaded 300 rows and 5 columns\n",
      "22-08-2020 23:56:23 - Loading data-64 file....\n",
      "Loaded 86 rows and 5 columns\n",
      "22-08-2020 23:56:23 - Loading data-65 file....\n",
      "Loaded 1126 rows and 5 columns\n",
      "22-08-2020 23:56:23 - Loading data-66 file....\n",
      "Loaded 239 rows and 5 columns\n",
      "22-08-2020 23:56:24 - Loading data-67 file....\n",
      "Loaded 967 rows and 5 columns\n",
      "22-08-2020 23:56:24 - Loading data-68 file....\n",
      "Loaded 693 rows and 5 columns\n",
      "22-08-2020 23:56:28 - Loading data-69 file....\n",
      "Loaded 51 rows and 5 columns\n",
      "22-08-2020 23:56:31 - Loading data-70 file....\n",
      "Loaded 341 rows and 5 columns\n",
      "22-08-2020 23:56:31 - Load completed. This operation has taken 59.03 s\n"
     ]
    }
   ],
   "source": [
    "# Load third dataset (analysis data)\n",
    "an_data = functions.load_ts(sc, 70, 'data') # the function load_ts already imputes missing values when loading the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the rows and columns with more than 30% of missing values and replace the rest with the mode of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "22-08-2020 23:57:20 - Working with missing values...\n",
      "Removing rows with more than 30% of missing data...\n",
      "0 rows removed\n",
      "\n",
      "Showing columns with missing values: \n",
      "                   count  density\n",
      "race                2273     0.02\n",
      "weight             98569     0.97\n",
      "payer_code         40256     0.40\n",
      "medical_specialty  49949     0.49\n",
      "diag_1                21     0.00\n",
      "diag_2               358     0.00\n",
      "diag_3              1423     0.01\n",
      "\n",
      "Removing rows with more than 30% of missing data (weight, payer_code, medical_specialty) ...\n",
      "\n",
      "23-08-2020 00:08:24 - Columns removed. \n",
      "New shape: (101766 rows x 47 columns)\n",
      "\n",
      "23-08-2020 00:08:25 - Replacing missing values...\n",
      "Replacing NAs values of attribute race by value Caucasian\n",
      "Replacing NAs values of attribute diag_1 by value 428\n",
      "Replacing NAs values of attribute diag_2 by value 276\n",
      "Replacing NAs values of attribute diag_3 by value 250\n",
      "\n",
      "23-08-2020 00:09:12 - Missing values removed.  This operation has taken 711.97 s\n"
     ]
    }
   ],
   "source": [
    "hist_data = functions.impute_na(hist_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cast IDs with numbers to String."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- encounter_id: string (nullable = true)\n",
      " |-- patient_nbr: string (nullable = true)\n",
      " |-- race: string (nullable = false)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- admission_type_id: string (nullable = true)\n",
      " |-- discharge_disposition_id: string (nullable = true)\n",
      " |-- admission_source_id: string (nullable = true)\n",
      " |-- time_in_hospital: integer (nullable = true)\n",
      " |-- num_lab_procedures: integer (nullable = true)\n",
      " |-- num_procedures: integer (nullable = true)\n",
      " |-- num_medications: integer (nullable = true)\n",
      " |-- number_outpatient: integer (nullable = true)\n",
      " |-- number_emergency: integer (nullable = true)\n",
      " |-- number_inpatient: integer (nullable = true)\n",
      " |-- diag_1: string (nullable = false)\n",
      " |-- diag_2: string (nullable = false)\n",
      " |-- diag_3: string (nullable = false)\n",
      " |-- number_diagnoses: integer (nullable = true)\n",
      " |-- max_glu_serum: string (nullable = true)\n",
      " |-- A1Cresult: string (nullable = true)\n",
      " |-- metformin: string (nullable = true)\n",
      " |-- repaglinide: string (nullable = true)\n",
      " |-- nateglinide: string (nullable = true)\n",
      " |-- chlorpropamide: string (nullable = true)\n",
      " |-- glimepiride: string (nullable = true)\n",
      " |-- acetohexamide: string (nullable = true)\n",
      " |-- glipizide: string (nullable = true)\n",
      " |-- glyburide: string (nullable = true)\n",
      " |-- tolbutamide: string (nullable = true)\n",
      " |-- pioglitazone: string (nullable = true)\n",
      " |-- rosiglitazone: string (nullable = true)\n",
      " |-- acarbose: string (nullable = true)\n",
      " |-- miglitol: string (nullable = true)\n",
      " |-- troglitazone: string (nullable = true)\n",
      " |-- tolazamide: string (nullable = true)\n",
      " |-- examide: string (nullable = true)\n",
      " |-- citoglipton: string (nullable = true)\n",
      " |-- insulin: string (nullable = true)\n",
      " |-- glyburide-metformin: string (nullable = true)\n",
      " |-- glipizide-metformin: string (nullable = true)\n",
      " |-- glimepiride-pioglitazone: string (nullable = true)\n",
      " |-- metformin-rosiglitazone: string (nullable = true)\n",
      " |-- metformin-pioglitazone: string (nullable = true)\n",
      " |-- change: string (nullable = true)\n",
      " |-- diabetesMed: string (nullable = true)\n",
      " |-- readmitted: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hist_data = hist_data.withColumn(\"encounter_id\",hist_data[\"encounter_id\"].cast(StringType())) \\\n",
    "                    .withColumn(\"discharge_disposition_id\",hist_data[\"discharge_disposition_id\"].cast(StringType())) \\\n",
    "                    .withColumn(\"admission_source_id\",hist_data[\"admission_source_id\"].cast(StringType())) \\\n",
    "                    .withColumn(\"admission_type_id\",hist_data[\"admission_type_id\"].cast(StringType())) \\\n",
    "                    .withColumn(\"patient_nbr\",hist_data[\"patient_nbr\"].cast(StringType()))\n",
    "\n",
    "hist_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of categorical variables: 37\t Percentage: 74.00%\n",
      "Number of numerical variables: 13\t Percentage: 26.00%\n"
     ]
    }
   ],
   "source": [
    "data_types = defaultdict(list)\n",
    "for entry in hist_data.schema.fields:\n",
    "     data_types[str(entry.dataType)].append(entry.name)\n",
    "        \n",
    "categorical_variables = len(data_types[\"StringType\"])\n",
    "print(\"Number of categorical variables: {}\\t Percentage: {:.2f}%\". format(categorical_variables, \\\n",
    "                                                                      100*(categorical_variables/len(hist_data.columns))))\n",
    "numerical_variables = len(data_types[\"IntegerType\"])\n",
    "print(\"Number of numerical variables: {}\\t Percentage: {:.2f}%\". format(numerical_variables, \\\n",
    "                                                                    100*(numerical_variables/len(hist_data.columns))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 50508)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/marta/anaconda3/lib/python3.7/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/home/marta/anaconda3/lib/python3.7/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/home/marta/anaconda3/lib/python3.7/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/home/marta/anaconda3/lib/python3.7/socketserver.py\", line 720, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 270, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 242, in poll\n",
      "    if func():\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 246, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 692, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "plt.hist(hist_data.select(\"metformin-pioglitazone\").toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the columns to be used in the machine learning model in a new variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['encounter_id', 'race', 'age', 'payer_code', 'medical_specialty']\n",
    "hist_data_aux = hist_data.drop(*columns_to_drop)\n",
    "\n",
    "hist_data_aux = hist_data_aux.select(['metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', 'acetohexamide', \n",
    "                                 'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone', 'rosiglitazone', 'acarbose', \n",
    "                                  'miglitol', 'troglitazone', 'tolazamide', 'examide', 'citoglipton', 'insulin', 'glyburide-metformin',\n",
    "                                 'glipizide-metformin', 'glimepiride-pioglitazone', 'readmitted'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map the necessary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## New dataset: transform to binary class\n",
    "mapping_expr = create_map([lit(x) for x in chain(*readmitted_mapping.items())])\n",
    "hist_data_aux = hist_data_aux.withColumn(\"readmitted\", mapping_expr.getItem(col(\"readmitted\"))) \n",
    "\n",
    "## Final dataset: admission type\n",
    "mapping_expr = create_map([lit(x) for x in chain(*admission_type_id_mapping.items())])\n",
    "hist_data = hist_data.withColumn(\"admission_type_id\", mapping_expr.getItem(col(\"admission_type_id\")))\n",
    "\n",
    "## Final dataset: \n",
    "mapping_expr = create_map([lit(x) for x in chain(*discharge_disposition_id_mapping.items())])\n",
    "hist_data = hist_data.withColumn(\"discharge_disposition_id\", mapping_expr.getItem(col(\"discharge_disposition_id\")))\n",
    "\n",
    "## Final dataset: \n",
    "mapping_expr = create_map([lit(x) for x in chain(*admission_source_id_mapping.items())])\n",
    "hist_data = hist_data.withColumn(\"admission_source_id\", mapping_expr.getItem(col(\"admission_source_id\")))\n",
    "\n",
    "## Final dataset: \n",
    "mapping_expr = create_map([lit(x) for x in chain(*meds_mapping.items())])\n",
    "hist_data = hist_data.withColumn(\"metformin\", mapping_expr.getItem(col(\"metformin\"))) \\\n",
    "                    .withColumn(\"repaglinide\", mapping_expr.getItem(col(\"repaglinide\"))) \\\n",
    "                    .withColumn(\"nateglinide\", mapping_expr.getItem(col(\"nateglinide\"))) \\\n",
    "                    .withColumn(\"chlorpropamide\", mapping_expr.getItem(col(\"chlorpropamide\"))) \\\n",
    "                    .withColumn(\"glimepiride\", mapping_expr.getItem(col(\"glimepiride\"))) \\\n",
    "                    .withColumn(\"acetohexamide\", mapping_expr.getItem(col(\"acetohexamide\"))) \\\n",
    "                    .withColumn(\"glipizide\", mapping_expr.getItem(col(\"glipizide\"))) \\\n",
    "                    .withColumn(\"glyburide\", mapping_expr.getItem(col(\"glyburide\"))) \\\n",
    "                    .withColumn(\"tolbutamide\", mapping_expr.getItem(col(\"tolbutamide\"))) \\\n",
    "                    .withColumn(\"pioglitazone\", mapping_expr.getItem(col(\"pioglitazone\"))) \\\n",
    "                    .withColumn(\"rosiglitazone\", mapping_expr.getItem(col(\"rosiglitazone\"))) \\\n",
    "                    .withColumn(\"acarbose\", mapping_expr.getItem(col(\"acarbose\"))) \\\n",
    "                    .withColumn(\"miglitol\", mapping_expr.getItem(col(\"miglitol\"))) \\\n",
    "                    .withColumn(\"troglitazone\", mapping_expr.getItem(col(\"troglitazone\"))) \\\n",
    "                    .withColumn(\"tolazamide\", mapping_expr.getItem(col(\"tolazamide\"))) \\\n",
    "                    .withColumn(\"examide\", mapping_expr.getItem(col(\"examide\"))) \\\n",
    "                    .withColumn(\"citoglipton\", mapping_expr.getItem(col(\"citoglipton\"))) \\\n",
    "                    .withColumn(\"insulin\", mapping_expr.getItem(col(\"insulin\"))) \\\n",
    "                    .withColumn(\"glyburide-metformin\", mapping_expr.getItem(col(\"glyburide-metformin\"))) \\\n",
    "                    .withColumn(\"glipizide-metformin\", mapping_expr.getItem(col(\"glipizide-metformin\"))) \\\n",
    "                    .withColumn(\"glimepiride-pioglitazone\", mapping_expr.getItem(col(\"glimepiride-pioglitazone\"))) \\\n",
    "                    .withColumn(\"metformin-rosiglitazone\", mapping_expr.getItem(col(\"metformin-rosiglitazone\"))) \\\n",
    "                    .withColumn(\"metformin-pioglitazone\", mapping_expr.getItem(col(\"metformin-pioglitazone\")))\n",
    "\n",
    "## Final dataset: \n",
    "mapping_expr = create_map([lit(x) for x in chain(*change_mapping.items())])\n",
    "hist_data = hist_data.withColumn(\"change\", mapping_expr.getItem(col(\"change\")))\n",
    "\n",
    "## Final dataset: \n",
    "mapping_expr = create_map([lit(x) for x in chain(*diabetesMed_mapping.items())])\n",
    "hist_data = hist_data.withColumn(\"diabetesMed\", mapping_expr.getItem(col(\"diabetesMed\")))\n",
    "\n",
    "## Final dataset: \n",
    "mapping_expr = create_map([lit(x) for x in chain(*readmitted_mapping.items())])\n",
    "hist_data = hist_data.withColumn(\"readmitted\", mapping_expr.getItem(col(\"readmitted\"))) \\\n",
    "                        .withColumnRenamed(\"readmitted\", \"readmitted_trunc\")\n",
    "\n",
    "## Final dataset: diagnostics \n",
    "hist_data = hist_data.withColumn(\"diag_1\", translate(col(\"diag_1\"), \".\", \"\")) \\\n",
    "                     .withColumn(\"diag_2\", translate(col(\"diag_2\"), \".\", \"\")) \\\n",
    "                     .withColumn(\"diag_3\", translate(col(\"diag_3\"), \".\", \"\"))\n",
    "\n",
    "df = ss.read.load(\"tfm-marta/data/CMS30_DESC_LONG_DX.csv\", format=\"csv\", sep=\";\", inferSchema=True)\n",
    "diag_mapping = df.toPandas().set_index('_c0').to_dict()['_c1']\n",
    "diag_mapping\n",
    "\n",
    "mapping_expr = create_map([lit(x) for x in chain(*diag_mapping.items())])\n",
    "hist_data = hist_data.withColumn(\"diag_1\", mapping_expr.getItem(col(\"diag_1\"))) \\\n",
    "                     .withColumn(\"diag_2\", mapping_expr.getItem(col(\"diag_2\"))) \\\n",
    "                     .withColumn(\"diag_3\", mapping_expr.getItem(col(\"diag_3\")))\n",
    "\n",
    "## Final dataset: add new column: date\n",
    "hist_data = hist_data.withColumn('date', lit(datetime.now().strftime(\"%d-%m-%Y\")))\n",
    "hist_data = hist_data.withColumn('date', to_timestamp(unix_timestamp(hist_data.date, 'MM-dd-yyyy')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform categorical variables with more than 1 category using One-hot-encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## count number of categories of each categorical variable\n",
    "data_types = defaultdict(list)\n",
    "for entry in hist_data_aux.schema.fields:\n",
    "     data_types[str(entry.dataType)].append(entry.name)\n",
    "\n",
    "counts_summary = hist_data_aux.agg(*[countDistinct(c).alias(c) for c in data_types[\"StringType\"]])\n",
    "counts_summary = counts_summary.toPandas()\n",
    "\n",
    "counts = pd.Series(counts_summary.values.ravel())\n",
    "counts.index = counts_summary.columns\n",
    "\n",
    "sorted_vars = counts.sort_values(ascending = False) # number of categories of each variable\n",
    "ignore = list((sorted_vars[sorted_vars <=1]).index) # variables with less than 1 category\n",
    "# We'll ignore the categorical variables with only 1 category\n",
    "\n",
    "strings_used = [var for var in data_types[\"StringType\"] if var not in ignore]\n",
    "stage_string = [StringIndexer(inputCol= c, outputCol= c+\"_string_encoded\") for c in strings_used]\n",
    "stage_one_hot = [OneHotEncoder(inputCol= c+\"_string_encoded\", outputCol= c+ \"_one_hot\") for c in strings_used]\n",
    "\n",
    "ppl = Pipeline(stages= stage_string + stage_one_hot)\n",
    "hist_data_trunc = ppl.fit(hist_data_aux).transform(hist_data_aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all the features into a single vector for each observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data_types[\"IntegerType\"] + [var + \"_one_hot\" for var in strings_used]\n",
    "hist_data_trunc = VectorAssembler(inputCols = [x for x in features if x not in 'readmitted'], outputCol= \"features\")\\\n",
    "                .transform(hist_data_trunc).select(['readmitted', 'features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into training, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_hist: 49690 x 2\t val_hist: 21370 x 2\t test_hist: 30706 x 2\n"
     ]
    }
   ],
   "source": [
    "## Split data into train, validation and test\n",
    "train_histC_trunc, test_hist_trunc = functions.df_split(hist_data_trunc, 0.7)\n",
    "train_hist_trunc, val_hist_trunc = functions.df_split(train_histC_trunc, 0.7)\n",
    "\n",
    "## Split data into X and Y coordinates\n",
    "train_hist_trunc, val_hist_trunc =  functions.df_split(train_histC_trunc, 0.7)\n",
    "\n",
    "print(\"train_hist: {0} x {1}\\t val_hist: {2} x {3}\\t test_hist: {4} x {5}\"\\\n",
    "      .format(train_hist_trunc.count(), len(train_hist_trunc.columns),\n",
    "              val_hist_trunc.count(), len(val_hist_trunc.columns),\n",
    "              test_hist_trunc.count(), len(test_hist_trunc.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the rows and columns with more than 30% of missing values and replace the rest with the mode of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "22-08-2020 23:12:57 - Working with missing values...\n",
      "Removing rows with more than 30% of missing data...\n",
      "0 rows removed\n",
      "\n",
      "Showing columns with missing values: \n",
      "Empty DataFrame\n",
      "Columns: [count, density]\n",
      "Index: []\n",
      "\n",
      "Removing rows with more than 30% of missing data () ...\n",
      "\n",
      "22-08-2020 23:13:02 - Columns removed. \n",
      "New shape: (768 rows x 9 columns)\n",
      "\n",
      "22-08-2020 23:13:02 - Replacing missing values...\n",
      "\n",
      "22-08-2020 23:13:03 - Missing values removed.  This operation has taken 5.98 s\n",
      "root\n",
      " |-- Pregnancies: integer (nullable = true)\n",
      " |-- Glucose: integer (nullable = true)\n",
      " |-- BloodPressure: integer (nullable = true)\n",
      " |-- SkinThickness: integer (nullable = true)\n",
      " |-- Insulin: integer (nullable = true)\n",
      " |-- BMI: double (nullable = true)\n",
      " |-- DiabetesPedigreeFunction: double (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Class: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_data = functions.impute_na(pred_data)\n",
    "pred_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into validation (21%), training (49%) and test (30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain_pred: 368 x 8\t ytrain_pred: 368 x 1\n",
      "Xval_pred: 165 x 8\t yval_pred: 165 x 1\n",
      "Xtest_pred: 235 x 8\t ytest_pred: 235 x 1\n"
     ]
    }
   ],
   "source": [
    "## Split data into train, validation and test\n",
    "train_predC, test_pred = functions.df_split(pred_data, 0.7)\n",
    "train_pred, val_pred = functions.df_split(train_predC, 0.7)\n",
    "\n",
    "## Split data into X and Y coordinates\n",
    "Xtrain_pred, ytrain_pred = train_pred.select(train_pred.columns[0:len(train_pred.columns)-1]), train_pred.select('Class')\n",
    "Xval_pred, yval_pred =  val_pred.select(val_pred.columns[0:len(val_pred.columns)-1]), val_pred.select('Class')\n",
    "Xtest_pred, ytest_pred = test_pred.select(test_pred.columns[0:len(test_pred.columns)-1]), test_pred.select('Class')\n",
    "\n",
    "print(\"Xtrain_pred: {0} x {1}\\t ytrain_pred: {2} x {3}\".format(Xtrain_pred.count(), len(Xtrain_pred.columns),\n",
    "                                                              ytrain_pred.count(), len(ytrain_pred.columns)))\n",
    "print(\"Xval_pred: {0} x {1}\\t yval_pred: {2} x {3}\".format(Xval_pred.count(), len(Xval_pred.columns),\n",
    "                                                              yval_pred.count(), len(yval_pred.columns)))\n",
    "print(\"Xtest_pred: {0} x {1}\\t ytest_pred: {2} x {3}\".format(Xtest_pred.count(), len(Xtest_pred.columns),\n",
    "                                                              ytest_pred.count(), len(ytest_pred.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all the features into a single vector for each observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|Class|            features|\n",
      "+-----+--------------------+\n",
      "|    0|[0.0,86.0,68.0,32...|\n",
      "|    0|[0.0,91.0,80.0,0....|\n",
      "|    0|[0.0,95.0,64.0,39...|\n",
      "+-----+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Put together all the input attributes.\n",
    "assembler = VectorAssembler(inputCols=[x for x in train_predC.columns if x not in 'Class'], outputCol='features')\n",
    "\n",
    "train_predC_trunc = assembler.transform(train_predC).select(['Class', 'features'])\n",
    "train_pred_trunc = assembler.transform(train_pred).select(['Class', 'features'])\n",
    "val_pred_trunc = assembler.transform(val_pred).select(['Class', 'features'])\n",
    "test_pred_trunc = assembler.transform(test_pred).select(['Class', 'features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add columns: date and patient number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data = pred_data.withColumn(\"new_column\",lit(\"ABC\"))\n",
    "w = Window().partitionBy('new_column').orderBy(lit('A'))\n",
    "pred_data = pred_data.withColumn(\"patient_nbr\", 50125+row_number().over(w)).drop(\"new_column\")\n",
    "pred_data = pred_data.withColumn(\"patient_nbr\",pred_data[\"patient_nbr\"].cast(StringType()))\n",
    "pred_data = pred_data.withColumn('date', lit(datetime.now().strftime(\"%d-%m-%Y\")))\n",
    "pred_data = pred_data.withColumn('date', to_timestamp(unix_timestamp(pred_data.date, 'MM-dd-yyyy')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Analytical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "an_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "an_data = an_data.withColumn(\"value\",an_data[\"value\"].cast(IntegerType())) \n",
    "\n",
    "\n",
    "mapping_expr = create_map([lit(x) for x in chain(*mapping.items())])\n",
    "an_data = an_data.withColumn(\"code\", mapping_expr.getItem(col(\"code\"))) \\\n",
    "                 .withColumn(\"patient_nbr\",an_data[\"patient_nbr\"].cast(StringType())) \\\n",
    "                 .withColumn('date', an_data.date + expr('INTERVAL 29 years'))\n",
    "an_data.printSchema()\n",
    "an_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing and predicting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"metformin\").distinct().rdd.map(lambda r: r[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.55\n",
      "Validation accuracy: 0.55\n",
      "Test accuracy: 0.55\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(maxIter=1000, regParam=0, elasticNetParam=0.8, featuresCol=\"features\", labelCol=\"readmitted\")\n",
    "model = lr.fit(train_hist_trunc)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='readmitted')\n",
    "\n",
    "y_train_hist = model.transform(train_hist_trunc)\n",
    "train_accuracy = evaluator.evaluate(y_train_hist)\n",
    "print(\"Training accuracy: {:.2f}\".format(train_accuracy))\n",
    "\n",
    "y_val_hist = model.transform(val_hist_trunc)\n",
    "val_accuracy = evaluator.evaluate(y_val_hist)\n",
    "print(\"Validation accuracy: {:.2f}\".format(val_accuracy))\n",
    "\n",
    "y_test = model.transform(test_hist_trunc)\n",
    "test_accuracy = evaluator.evaluate(y_test)\n",
    "print(\"Test accuracy: {:.2f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add column with the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- encounter_id: string (nullable = true)\n",
      " |-- patient_nbr: string (nullable = true)\n",
      " |-- race: string (nullable = false)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- admission_type_id: string (nullable = true)\n",
      " |-- discharge_disposition_id: string (nullable = true)\n",
      " |-- admission_source_id: string (nullable = true)\n",
      " |-- time_in_hospital: integer (nullable = true)\n",
      " |-- num_lab_procedures: integer (nullable = true)\n",
      " |-- num_procedures: integer (nullable = true)\n",
      " |-- num_medications: integer (nullable = true)\n",
      " |-- number_outpatient: integer (nullable = true)\n",
      " |-- number_emergency: integer (nullable = true)\n",
      " |-- number_inpatient: integer (nullable = true)\n",
      " |-- diag_1: string (nullable = true)\n",
      " |-- diag_2: string (nullable = true)\n",
      " |-- diag_3: string (nullable = true)\n",
      " |-- number_diagnoses: integer (nullable = true)\n",
      " |-- max_glu_serum: string (nullable = true)\n",
      " |-- A1Cresult: string (nullable = true)\n",
      " |-- metformin: integer (nullable = true)\n",
      " |-- repaglinide: integer (nullable = true)\n",
      " |-- nateglinide: integer (nullable = true)\n",
      " |-- chlorpropamide: integer (nullable = true)\n",
      " |-- glimepiride: integer (nullable = true)\n",
      " |-- acetohexamide: integer (nullable = true)\n",
      " |-- glipizide: integer (nullable = true)\n",
      " |-- glyburide: integer (nullable = true)\n",
      " |-- tolbutamide: integer (nullable = true)\n",
      " |-- pioglitazone: integer (nullable = true)\n",
      " |-- rosiglitazone: integer (nullable = true)\n",
      " |-- acarbose: integer (nullable = true)\n",
      " |-- miglitol: integer (nullable = true)\n",
      " |-- troglitazone: integer (nullable = true)\n",
      " |-- tolazamide: integer (nullable = true)\n",
      " |-- examide: integer (nullable = true)\n",
      " |-- citoglipton: integer (nullable = true)\n",
      " |-- insulin: integer (nullable = true)\n",
      " |-- glyburide-metformin: integer (nullable = true)\n",
      " |-- glipizide-metformin: integer (nullable = true)\n",
      " |-- glimepiride-pioglitazone: integer (nullable = true)\n",
      " |-- metformin-rosiglitazone: integer (nullable = true)\n",
      " |-- metformin-pioglitazone: integer (nullable = true)\n",
      " |-- change: integer (nullable = true)\n",
      " |-- diabetesMed: integer (nullable = true)\n",
      " |-- readmitted_trunc: integer (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- Accuracy: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hist_data = hist_data.withColumn(\"Accuracy\", lit(test_accuracy))\n",
    "\n",
    "hist_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction data\n",
    "\n",
    "blablablablabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: 0.001\n",
      "Training accuracy: 0.85\n",
      "Validation accuracy: 0.84\n",
      "Test accuracy: 0.80\n"
     ]
    }
   ],
   "source": [
    "model = LinearSVC(labelCol='Class')\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(model.regParam, [1.0, 0.1, 0.01, 0.001, 0.0001, 0.0]) \\\n",
    "        .build()\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='Class')\n",
    "crossval = CrossValidator(estimator=model, estimatorParamMaps=paramGrid, evaluator=evaluator, parallelism=2, numFolds=5)\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(train_pred_trunc)\n",
    "print(\"Regularization parameter: {}\".format(cvModel.bestModel._java_obj.getRegParam()))\n",
    "\n",
    "y_train_pred = cvModel.transform(train_pred_trunc)\n",
    "train_accuracy = evaluator.evaluate(y_train_pred)\n",
    "print(\"Training accuracy: {:.2f}\".format(train_accuracy))\n",
    "\n",
    "y_val_pred = cvModel.transform(val_pred_trunc)\n",
    "val_accuracy = evaluator.evaluate(y_val_pred)\n",
    "print(\"Validation accuracy: {:.2f}\".format(val_accuracy))\n",
    "\n",
    "### Final model\n",
    "model = LinearSVC(labelCol='Class', regParam=cvModel.bestModel._java_obj.getRegParam()).fit(train_predC_trunc)\n",
    "y_test = model.transform(test_pred_trunc)\n",
    "test_accuracy = evaluator.evaluate(y_test)\n",
    "print(\"Test accuracy: {:.2f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-----+-----------+----+------------------+\n",
      "|Pregnancies|Glucose|BloodPressure|SkinThickness|Insulin| BMI|DiabetesPedigreeFunction|Age|Class|patient_nbr|date|          Accuracy|\n",
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-----+-----------+----+------------------+\n",
      "|          6|    148|           72|           35|      0|33.6|                   0.627| 50|    1|      50126|null|0.7998110614908964|\n",
      "|          1|     85|           66|           29|      0|26.6|                   0.351| 31|    0|      50127|null|0.7998110614908964|\n",
      "|          8|    183|           64|            0|      0|23.3|                   0.672| 32|    1|      50128|null|0.7998110614908964|\n",
      "|          1|     89|           66|           23|     94|28.1|                   0.167| 21|    0|      50129|null|0.7998110614908964|\n",
      "|          0|    137|           40|           35|    168|43.1|                   2.288| 33|    1|      50130|null|0.7998110614908964|\n",
      "|          5|    116|           74|            0|      0|25.6|                   0.201| 30|    0|      50131|null|0.7998110614908964|\n",
      "|          3|     78|           50|           32|     88|31.0|                   0.248| 26|    1|      50132|null|0.7998110614908964|\n",
      "|         10|    115|            0|            0|      0|35.3|                   0.134| 29|    0|      50133|null|0.7998110614908964|\n",
      "|          2|    197|           70|           45|    543|30.5|                   0.158| 53|    1|      50134|null|0.7998110614908964|\n",
      "|          8|    125|           96|            0|      0| 0.0|                   0.232| 54|    1|      50135|null|0.7998110614908964|\n",
      "|          4|    110|           92|            0|      0|37.6|                   0.191| 30|    0|      50136|null|0.7998110614908964|\n",
      "|         10|    168|           74|            0|      0|38.0|                   0.537| 34|    1|      50137|null|0.7998110614908964|\n",
      "|         10|    139|           80|            0|      0|27.1|                   1.441| 57|    0|      50138|null|0.7998110614908964|\n",
      "|          1|    189|           60|           23|    846|30.1|                   0.398| 59|    1|      50139|null|0.7998110614908964|\n",
      "|          5|    166|           72|           19|    175|25.8|                   0.587| 51|    1|      50140|null|0.7998110614908964|\n",
      "|          7|    100|            0|            0|      0|30.0|                   0.484| 32|    1|      50141|null|0.7998110614908964|\n",
      "|          0|    118|           84|           47|    230|45.8|                   0.551| 31|    1|      50142|null|0.7998110614908964|\n",
      "|          7|    107|           74|            0|      0|29.6|                   0.254| 31|    1|      50143|null|0.7998110614908964|\n",
      "|          1|    103|           30|           38|     83|43.3|                   0.183| 33|    0|      50144|null|0.7998110614908964|\n",
      "|          1|    115|           70|           30|     96|34.6|                   0.529| 32|    1|      50145|null|0.7998110614908964|\n",
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-----+-----------+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_data = pred_data.withColumn(\"Accuracy\", lit(test_accuracy))\n",
    "pred_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis data\n",
    "\n",
    "blablablablabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing data into elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myHost = \"192.168.146.131\"\n",
    "\n",
    "es = Elasticsearch([{\"host\": myHost,\"port\":9200}]) # Connect to the elastic cluster\n",
    "print(es)\n",
    "\n",
    "def uploadToES(indexName, indexBody, docType=None, requestBody=None, verbose=True):\n",
    "    if (es.indices.exists(indexName)==False): \n",
    "        es.indices.create(index=indexName, body=requestBody)\n",
    "        if(verbose): print(\"creating {} index...\".format(indexName))\n",
    "    es.index(index=indexName,doc_type=docType, body=indexBody) # Store the document in Elasticsearch\n",
    "    if(verbose): print(\"adding document {} to {} index...\".format(indexBody, indexName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux=hist_data.limit(200).toPandas()\n",
    "for i in range(0, aux.shape[0]):\n",
    "    body=aux.iloc[i].to_json(orient=\"index\", date_format='iso')\n",
    "    uploadToES(\"tfm-marta-historical-data\", body)\n",
    "\n",
    "\n",
    "aux=pred_data.toPandas()\n",
    "for i in range(0, aux.shape[0]):\n",
    "    body=aux.iloc[i].to_json(orient=\"index\", date_format='iso')\n",
    "    uploadToES(\"tfm-marta-prediction-data\", body)\n",
    "    \n",
    "# an_requestBody = {\n",
    "#     \"mappings\": {\n",
    "#         \"properties\": {\n",
    "#             \"date\": {\"type\": \"date\", \"format\":\"yyyy-MM-dd HH:mm || yyyy-MM-dd || epoch_millis\", \"ignore_malformed\":True},\n",
    "#             \"code\": {\"type\": \"keyword\"},\n",
    "#             \"value\":{\"type\": \"long\"}\n",
    "#         }\n",
    "#     }\n",
    "# }\n",
    "\n",
    "aux=an_data.toPandas()    \n",
    "for j in range(0, aux.shape[0]):   \n",
    "    body=aux.iloc[j].to_json(orient=\"index\", date_format='iso')\n",
    "    uploadToES(\"tfm-marta-analysis-data\", body)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
